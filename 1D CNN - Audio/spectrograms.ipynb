{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "spectrograms (1).ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNCgITYlZ9YZ",
        "outputId": "1ecefd78-3b66-46e3-e328-31e7d5d6d826"
      },
      "source": [
        "pip install Image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keyring is skipped due to an exception: org.freedesktop.DBus.Error.FileNotFound: Failed to connect to socket /run/user/1080/bus: No such file or directory\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: Image in /home/dcslab23/.local/lib/python3.6/site-packages (1.5.33)\n",
            "Requirement already satisfied: pillow in /home/dcslab23/.local/lib/python3.6/site-packages (from Image) (8.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from Image) (1.15.0)\n",
            "Requirement already satisfied: django in /home/dcslab23/.local/lib/python3.6/site-packages (from Image) (3.2.3)\n",
            "Requirement already satisfied: asgiref<4,>=3.3.2 in /home/dcslab23/.local/lib/python3.6/site-packages (from django->Image) (3.3.4)\n",
            "Requirement already satisfied: sqlparse>=0.2.2 in /home/dcslab23/.local/lib/python3.6/site-packages (from django->Image) (0.4.1)\n",
            "Requirement already satisfied: pytz in /home/dcslab23/.local/lib/python3.6/site-packages (from django->Image) (2021.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from asgiref<4,>=3.3.2->django->Image) (3.7.4.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj2LIYEmZ9Yh",
        "outputId": "5c965d66-33d9-40cb-b19f-e202a3a0eda7"
      },
      "source": [
        "pip install scipy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keyring is skipped due to an exception: org.freedesktop.DBus.Error.FileNotFound: Failed to connect to socket /run/user/1080/bus: No such file or directory\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: scipy in /home/dcslab23/.local/lib/python3.6/site-packages (1.5.4)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.19.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQQ4OVumZ9Yi"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.lib import stride_tricks\n",
        "import os\n",
        "from PIL import Image\n",
        "import scipy.io.wavfile as wav\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61EnIOPaaRc4"
      },
      "source": [
        "This script creates spectrogram matrices from wav files that can be passed \n",
        "to the CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzWK6yVUZ9Yj"
      },
      "source": [
        "prefix = '/content/drive/MyDrive/Colab Notebooks/Depression Detection Code Workspace/Dataset/'\n",
        "df_train = pd.read_csv('train_split_Depression_AVEC2017.csv')\n",
        "\n",
        "df_test = pd.read_csv('dev_split_Depression_AVEC2017.csv')\n",
        "\n",
        "df_dev = pd.concat([df_train, df_test], axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT9K53mCZ9Yk"
      },
      "source": [
        "def stft(sig, frameSize, overlapFac=0.5, window=np.hanning):\n",
        "    \"\"\"\n",
        "    Short-time Fourier transform of audio signal.\n",
        "    \"\"\"\n",
        "    win = window(frameSize)\n",
        "    hopSize = int(frameSize - np.floor(overlapFac * frameSize))\n",
        "    samples = np.append(np.zeros((np.floor(frameSize/2.0).astype(int))), sig)\n",
        "    cols = np.ceil((len(samples) - frameSize) / float(hopSize)) + 1\n",
        "    cols=cols.astype(int)\n",
        "    samples = np.append(samples, np.zeros(frameSize))\n",
        "    frames = stride_tricks.as_strided(samples, shape=(cols, frameSize),\n",
        "                                      strides=(samples.strides[0]*hopSize,\n",
        "                                      samples.strides[0])).copy()\n",
        "    frames *= win\n",
        "\n",
        "    return np.fft.rfft(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqPN1hhTZ9Yl"
      },
      "source": [
        "def logscale_spec(spec, sr=44100, factor=20.):\n",
        "    \"\"\"\n",
        "    Scale frequency axis logarithmically.\n",
        "    \"\"\"\n",
        "    timebins, freqbins = np.shape(spec)\n",
        "\n",
        "    scale = np.linspace(0, 1, freqbins) ** factor\n",
        "    scale *= (freqbins-1)/max(scale)\n",
        "    scale = np.unique(np.round(scale))\n",
        "    newspec = np.complex128(np.zeros([timebins, len(scale)]))\n",
        "    for i in range(0, len(scale)):\n",
        "        if i == len(scale)-1:\n",
        "            newspec[:, i] = np.sum(spec[:, int(scale[i]):], axis=1)\n",
        "        else:\n",
        "            newspec[:, i] = np.sum(spec[:,int(scale[i]):int(scale[i+1])], axis=1)\n",
        "    allfreqs = np.abs(np.fft.fftfreq(freqbins*2, 1./sr)[:freqbins+1])\n",
        "    freqs = []\n",
        "    for i in range(0, len(scale)):\n",
        "        if i == len(scale)-1:\n",
        "            freqs += [np.mean(allfreqs[int(scale[i]):])]\n",
        "        else:\n",
        "            freqs += [np.mean(allfreqs[int(scale[i]):int(scale[i+1])])]\n",
        "\n",
        "    return newspec, freqs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r5AE5AfZ9Ym"
      },
      "source": [
        "def stft_matrix(audiopath, binsize=2**10, png_name='tmp.png',\n",
        "                save_png=False, offset=0):\n",
        "    \"\"\"\n",
        "    A function that converts a wav file into a spectrogram represented by a \\\n",
        "    matrix where rows represent frequency bins, columns represent time, and \\\n",
        "    the values of the matrix represent the decibel intensity. A matrix of \\\n",
        "    this form can be passed as input to the CNN after undergoing normalization.\n",
        "    \"\"\"\n",
        "    samplerate, samples = wav.read(audiopath)\n",
        "    s = stft(samples, binsize)\n",
        "    sshow, freq = logscale_spec(s, factor=1, sr=samplerate)\n",
        "    ims = 20.*np.log10(np.abs(sshow)/10e-6)  \n",
        "    timebins, freqbins = np.shape(ims)\n",
        "    ims = np.transpose(ims)\n",
        "    ims = np.flipud(ims) \n",
        "    if save_png:\n",
        "        create_png(ims, png_name)\n",
        "\n",
        "    return ims\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZmZwxYha0aC"
      },
      "source": [
        "# Spectrogram_Dictionaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZXuePXaaqju"
      },
      "source": [
        "This script builds dictionaries for the depressed and non-depressed classes\n",
        "with each participant id as the key, and the associated segmented matrix\n",
        "spectrogram representation as the value. Said values can than be cropped and\n",
        "randomly sampled as input to the CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SR2wnDjZ9Yn"
      },
      "source": [
        "def build_class_dictionaries(dir_name):\n",
        "    \"\"\"\n",
        "    Builds a dictionary of depressed participants and non-depressed\n",
        "    participants with the participant id as the key and the matrix\n",
        "    representation of the no_silence wav file as the value. These\n",
        "    values of this dictionary are then randomly cropped and sampled\n",
        "    from to create balanced class and speaker inputs to the CNN.\n",
        "    Parameters\n",
        "    ----------\n",
        "    dir_name : filepath\n",
        "        directory containing participant's folders (which contains the\n",
        "        no_silence.wav)\n",
        "    Returns\n",
        "    -------\n",
        "    depressed_dict : dictionary\n",
        "        dictionary of depressed individuals with keys of participant id\n",
        "        and values of with the matrix spectrogram representation\n",
        "    normal_dict : dictionary\n",
        "        dictionary of non-depressed individuals with keys of participant id\n",
        "        and values of with the matrix spectrogram representation\n",
        "    \"\"\"\n",
        "    depressed_dict = dict()\n",
        "    normal_dict = dict()\n",
        "    for subdir, dirs, files in os.walk(dir_name):\n",
        "        for file in files:\n",
        "            if file.endswith('no_silence.wav'):\n",
        "                partic_id = int(file.split('_')[0][1:])\n",
        "                if in_dev_split(partic_id):\n",
        "                    wav_file = os.path.join(subdir, file)\n",
        "                    mat = stft_matrix(wav_file)\n",
        "                    depressed = get_depression_label(partic_id) \n",
        "                    if depressed:\n",
        "                        depressed_dict[partic_id] = mat\n",
        "                    elif not depressed:\n",
        "                        normal_dict[partic_id] = mat\n",
        "    return depressed_dict, normal_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wE4kLOaWZ9Yo"
      },
      "source": [
        "def in_dev_split(partic_id):\n",
        "    \"\"\"\n",
        "    Returns True if the participant is in the AVEC development split\n",
        "    (aka participant's we have depression labels for)\n",
        "    \"\"\"\n",
        "    return partic_id in set(df_dev['Participant_ID'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dELtgrH3Z9Yo"
      },
      "source": [
        "def get_depression_label(partic_id):\n",
        "    \"\"\"\n",
        "    Returns participant's PHQ8 Binary label. 1 representing depression;\n",
        "    0 representing no depression.\n",
        "    \"\"\"\n",
        "    return df_dev.loc[df_dev['Participant_ID'] ==\n",
        "                      partic_id]['PHQ8_Binary'].item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-vW-daoZ9Yp",
        "outputId": "48c5df3f-d27f-4594-d17f-0cf3701bbe1e"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    dir_name = os.path.dirname(os.path.realpath(\"segmented_audio\"))\n",
        "    depressed_dict, normal_dict = build_class_dictionaries(dir_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log10\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log10\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log10\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log10\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5_B9gmOZ9Yq",
        "outputId": "58ec66a0-dee7-4b44-f20a-e792220519cb"
      },
      "source": [
        "print(\"depressed_dict: \",len(depressed_dict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "depressed_dict:  36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-Qn4XdZZ9Yq",
        "outputId": "4c2427f6-77d6-4a27-93f0-68779bc2f9c4"
      },
      "source": [
        "print(\"normal_dict:\",len(normal_dict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normal_dict: 61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmEDjZISZ9Yr"
      },
      "source": [
        "Random_Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRJKer2YZ9Yr",
        "outputId": "348efc2b-b3bc-4f53-cf2f-3ca2ed898ad2"
      },
      "source": [
        "pip install boto"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keyring is skipped due to an exception: org.freedesktop.DBus.Error.FileNotFound: Failed to connect to socket /run/user/1080/bus: No such file or directory\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: boto in /home/dcslab23/.local/lib/python3.6/site-packages (2.49.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eyrlZ3hZ9Ys"
      },
      "source": [
        "import boto\n",
        "import numpy as np\n",
        "import os\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOR_hNFWbHOq"
      },
      "source": [
        "There exists a large data imbalance between positive and negative samples,\n",
        "which incurs a large bias in classification. The number of non-depressed\n",
        "subjects is about four times bigger than that of depressed ones. If these\n",
        "samples for learning, the model will have a strong bias to the non-depressed\n",
        "class. Moreover, regarding the length of each sample, a much longer signal of\n",
        "an individual may emphasize some characteristics that are person specific.\n",
        "To solve the problem, I perform random cropping on each of the participant's\n",
        "spectrograms of a specified width (time) and constant height (frequency), to\n",
        "ensure the CNN has an equal proportion for every subject and each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me8DCE1zZ9Ys"
      },
      "source": [
        "np.random.seed(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7CoHwYTZ9Yt"
      },
      "source": [
        "\n",
        "def determine_num_crops(depressed_dict, normal_dict, crop_width=125):\n",
        "    \"\"\"\n",
        "    Finds the shortest clip in the entire dataset which, according to our\n",
        "    random sampling strategy, will limit the number of samples we take from\n",
        "    each clip to make sure our classes are balanced.\n",
        "    Parameters\n",
        "    ----------\n",
        "    depressed_dict : dictionary\n",
        "        a dictionary of depressed participants with the participant id as the\n",
        "        key and the segmented and concatenated matrix representation of\n",
        "        their spectrograms as the values.\n",
        "    crop_width : integer\n",
        "        the desired pixel width of the crop samples\n",
        "        (125 pixels = 4 seconds of audio)\n",
        "    Returns\n",
        "    -------\n",
        "    num_samples_from_clips : int\n",
        "        the maximum number of samples that should be sampled from each clip\n",
        "        to ensure balanced classes can be built.\n",
        "    \"\"\"\n",
        "    merged_dict = dict(normal_dict, **{str(k): v for k, v in depressed_dict.items()})\n",
        "    shortest_clip = min(merged_dict.items(), key=lambda x: x[1].shape[1])\n",
        "    shortest_pixel_width = shortest_clip[1].shape[1]\n",
        "    num_samples_from_clips = shortest_pixel_width / crop_width\n",
        "    return num_samples_from_clips\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVB5fPWgZ9Yu"
      },
      "source": [
        "def build_class_sample_dict(segmented_audio_dict, n_samples, crop_width):\n",
        "    \"\"\"\n",
        "    Get N (num_samples) pseudo random non-overlapping samples from the all\n",
        "    the depressed participants.\n",
        "    Parameters\n",
        "    ----------\n",
        "    segmented_audio_dict : dictionary\n",
        "        a dictionary of a class of participants with keys of participant ids\n",
        "        and values of the segmented audio matrix spectrogram representation\n",
        "    n_samples : integer\n",
        "        number of random non-overlapping samples to extract from each\n",
        "        segmented audio matrix spectrogram\n",
        "    crop_width : integer\n",
        "        the desired pixel width of the crop samples\n",
        "        (125 pixels = 4 seconds of audio)\n",
        "    Returns\n",
        "    -------\n",
        "    class sample dict : dictionary\n",
        "        a dictionary of a class of participants with keys of participant ids\n",
        "        and values of a list of the cropped samples from the spectrogram\n",
        "        matrices. The lists are n_samples long and the entries within the\n",
        "        list have dimension (numFrequencyBins * crop_width)\n",
        "    \"\"\"\n",
        "    class_samples_dict = dict()\n",
        "    for partic_id, clip_mat in segmented_audio_dict.items():\n",
        "            samples = get_random_samples(clip_mat, n_samples, crop_width)\n",
        "            class_samples_dict[partic_id] = samples\n",
        "    return class_samples_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxJ__pB6Z9Yv"
      },
      "source": [
        "def get_random_samples(matrix, n_samples, crop_width):\n",
        "    \"\"\"\n",
        "    Get N random samples with width of crop_width from the numpy matrix\n",
        "    representing the participant's audio spectrogram.\n",
        "    \"\"\"\n",
        "    clipped_mat = matrix[:, (matrix.shape[1] % crop_width):]\n",
        "    n_splits = clipped_mat.shape[1] / crop_width\n",
        "    #print(\"clipped_mat\",type(clipped_mat))\n",
        "    #print(\"n_splits\",type(n_splits))\n",
        "    cropped_sample_ls = np.split(clipped_mat, n_splits, axis=1)\n",
        "    #print(\"cropped_sample_ls\",type(cropped_sample_ls))\n",
        "    #print(\"n_samples\",type(n_samples))\n",
        "    samples = random.sample(cropped_sample_ls, int(n_samples))\n",
        "    return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcU5NhLKZ9Yw"
      },
      "source": [
        "def create_sample_dicts(crop_width):\n",
        "    \"\"\"\n",
        "    Utilizes the above function to return two dictionaries, depressed\n",
        "    and normal. Each dictionary has only participants in the specific class,\n",
        "    with participant ids as key, a values of a list of the cropped samples\n",
        "    from the spectrogram matrices. The lists are vary in length depending\n",
        "    on the length of the interview clip. The entries within the list are\n",
        "    numpy arrays with dimennsion (513, 125).\n",
        "    \"\"\"\n",
        "    # build dictionaries of participants and segmented audio matrix\n",
        "    #dir_name = os.path.dirname(os.path.realpath(\"segmented_audio\"))\n",
        "    #depressed_dict, normal_dict = build_class_dictionaries(dir_name)\n",
        "    n_samples = determine_num_crops(depressed_dict, normal_dict,\n",
        "                                    crop_width=crop_width)\n",
        "    depressed_samples = build_class_sample_dict(depressed_dict, n_samples,\n",
        "                                                crop_width)\n",
        "    normal_samples = build_class_sample_dict(normal_dict, n_samples,\n",
        "                                             crop_width)\n",
        "    for key, _ in depressed_samples.items():\n",
        "        path = 'Randomly_Sampled_Data/'\n",
        "        filename = 'D{}.npz'.format(key)\n",
        "        outfile = path + filename\n",
        "        np.savez(outfile, *depressed_samples[key])\n",
        "    for key, _ in normal_samples.items():\n",
        "        path = 'Randomly_Sampled_Data/'\n",
        "        filename = '/N{}.npz'.format(key)\n",
        "        outfile = path + filename\n",
        "        np.savez(outfile, *normal_samples[key])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3W91e6TZ9Yx"
      },
      "source": [
        "def rand_samp_train_test_split(npz_file_dir):\n",
        "    \"\"\"\n",
        "    Given the cropped segments from each class and particpant, this fucntion\n",
        "    determines how many samples we can draw from each particpant and how many\n",
        "    participants we can draw from each class.\n",
        "    Parameters\n",
        "    ----------\n",
        "    npz_file_dir : directory\n",
        "        directory contain the\n",
        "    crop_width : integer\n",
        "        the desired pixel width of the crop samples\n",
        "        (125 pixels = 4 seconds of audio)\n",
        "    Returns\n",
        "    -------\n",
        "    num_samples_from_clips : int\n",
        "        the maximum number of samples that should be sampled from each clip\n",
        "        to ensure balanced classes can be built.\n",
        "    \"\"\"\n",
        "    npz_files = os.listdir(npz_file_dir)\n",
        "\n",
        "    dep_samps = [f for f in npz_files if f.startswith('D')]\n",
        "    norm_samps = [f for f in npz_files if f.startswith('N')]\n",
        "    max_samples = min(len(dep_samps), len(norm_samps))\n",
        "    dep_select_samps = np.random.choice(dep_samps, size=max_samples,\n",
        "                                        replace=False)\n",
        "    norm_select_samps = np.random.choice(norm_samps, size=max_samples,\n",
        "                                         replace=False)\n",
        "    test_size = 0.2\n",
        "    num_test_samples = int(len(dep_select_samps) * test_size)\n",
        "\n",
        "    train_samples = []\n",
        "    for sample in dep_select_samps[:-num_test_samples]:\n",
        "        npz_file = npz_file_dir + '/' + sample\n",
        "        with np.load(npz_file) as data:\n",
        "            for key in data.keys():\n",
        "                train_samples.append(data[key])\n",
        "    for sample in norm_select_samps[:-num_test_samples]:\n",
        "        npz_file = npz_file_dir + '/' + sample\n",
        "        with np.load(npz_file) as data:\n",
        "            for key in data.keys():\n",
        "                train_samples.append(data[key])\n",
        "    #y=(np.ones(len(train_samples)//2),np.zeros(len(train_samples)//2))\n",
        "    #print(\"y:\",y)\n",
        "    train_labels = np.concatenate((np.ones(len(train_samples)//2),\n",
        "                                   np.zeros(len(train_samples)//2)))\n",
        "    test_samples = []\n",
        "    for sample in dep_select_samps[-num_test_samples:]:\n",
        "        npz_file = npz_file_dir + '/' + sample\n",
        "        with np.load(npz_file) as data:\n",
        "            for key in data.keys():\n",
        "                test_samples.append(data[key])\n",
        "    for sample in norm_select_samps[-num_test_samples:]:\n",
        "        npz_file = npz_file_dir + '/' + sample\n",
        "        with np.load(npz_file) as data:\n",
        "            for key in data.keys():\n",
        "                test_samples.append(data[key])\n",
        "    test_labels = np.concatenate((np.ones(len(test_samples)//2),\n",
        "                                  np.zeros(len(test_samples)//2)))\n",
        "\n",
        "    return np.array(train_samples), train_labels, np.array(test_samples), \\\n",
        "        test_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ5ogoFhZ9Yy"
      },
      "source": [
        "def save_to_bucket(file, obj_name):\n",
        "    \"\"\"\n",
        "    Saves local file to S3 bucket for redundancy and reproducibility.\n",
        "    \"\"\"\n",
        "    conn = boto.connect_s3(access_key, access_secret_key)\n",
        "    bucket = conn.get_bucket('depression-detect')\n",
        "    file_object = bucket.new_key(obj_name)\n",
        "    file_object.set_contents_from_filename(file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oJ_nUM9Z9Yz",
        "outputId": "1c2cebbb-59b8-4305-ae36-6b29873c931d"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    create_sample_dicts(crop_width=125)\n",
        "    train_samples, train_labels, test_samples, \\\n",
        "        test_labels = rand_samp_train_test_split('Randomly_Sampled_Data')\n",
        "\n",
        "    # save as npz locally\n",
        "    print(\"Saving npz file locally...\")\n",
        "    np.savez('Randomly_Sampled_Data/train_samples.npz', train_samples)\n",
        "    np.savez('Randomly_Sampled_Data/train_labels.npz', train_labels)\n",
        "    np.savez('Randomly_Sampled_Data/test_samples.npz', test_samples)\n",
        "    np.savez('Randomly_Sampled_Data/test_labels.npz', test_labels)\n",
        "    print(\"Saved Locally\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving npz file locally...\n",
            "Saved Locally\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31IwiFj8Z9Y6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgCb_2BvZ9Y6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}